{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for topic extraction based on unsupervised learning \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic model aims to extract themes from large-scale unstructured corpus. Topic is the core idea expressed in corpus. From a statistical point of view, word frequency distribution is used to describe themes. A sentence, a paragraph and a document are generated from a topic-based and word-based probabilistic model. \n",
    "    \n",
    "In the field of subject model, the most commonly used algorithm is LDA (Latent Dirichlet Allocation), which was proposed by Blei et al. in 2003. Specifically, LDA is an unsupervised machine learning technology, which regards the nature of documents as a collection of potential topics, and each topic is based on word distribution. LDA uses the BoW (Bag of Words) method to transform each document to a word frequency vector, thus converting unstructured data into easy-to-build, numerical information of modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import related modules \n",
    "import #Word Segmentation Models\n",
    "import pandas as pd  # Used to process Excel files\n",
    "import pyLDAvis  # Topic model visualization tool  \n",
    "import pyLDAvis.sklearn  # pyLDAvis's sklearn interface, sklearn = Scikit-learn\n",
    "# TfidfVectorizer transforms the document into TF-IDF matrix, \n",
    "# and CountVectorizer transforms the document into the word frequency matrix. \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# online variational Bayes Algorithm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('file.csv', names=['sentence'])  # read file \n",
    "df.tail()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Word segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Word Segmentation\n",
    "# Function for word segmentation\n",
    "def word_cut(sentence):\n",
    "    return \" \".join(fool.cut(sentence))\n",
    "\n",
    "df['sentence_cutted'] = df['sentence'].apply(word_cut)\n",
    "df['sentence_cutted'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Remove the stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = list()\n",
    "# imput the stop word\n",
    "with open('stopwords.txt', 'r') as f:\n",
    "    for item in f.readlines():\n",
    "        stopwords.append(item.strip())\n",
    "stopwords = list(set(stopwords))\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Document matrix transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_features = 1000\n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',  # Three options for deleting accents ï¼šascii, unicode, None\n",
    "                                max_features=n_features,  # Consider only the first n words with the largest word frequency in the corpus.\n",
    "                                stop_words=stopwords,  # stop word can be list\n",
    "                                max_df=0.80,  # If a word appears in more than 50% of the corpus, then give up.  | 0.50 0.75 0.70\n",
    "                                min_df=10)  # The truncated value (cut-off) is an integer, which means that the word frequency is less than 10.  | 10 6 9\n",
    "tf = tf_vectorizer.fit_transform(df.sentence_cutted)  # Learn vocabulary dictionary and return document word matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_features = 1000\n",
    "tf_idf_vectorizer = TfidfVectorizer(strip_accents='unicode', \n",
    "                                    stop_words=stopwords, \n",
    "                                    max_df=0.80, \n",
    "                                    min_df=10, \n",
    "                                    max_features=n_features, \n",
    "                                    use_idf=True)\n",
    "tf_idf = tf_idf_vectorizer.fit_transform(df.sentence_cutted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Topic model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n_components = 4  # Number of topics \n",
    "lda = LatentDirichletAllocation(n_components=n_components, \n",
    "                                max_iter=50,  # Maximum iterations \n",
    "                                learning_method='online',  # Two options: batch and online\n",
    "                                learning_offset=50., \n",
    "                                random_state=0, \n",
    "                                n_jobs=-1)\n",
    "lda.fit(tf)  # Learn from TF (matrix) and build a model. \n",
    "lda.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.View the theme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print('theme #%d:'%topic_idx)\n",
    "        print(' '.join([feature_names[i] for i in topic.argsort()[: -n_top_words-1: -1]]))\n",
    "    print()\n",
    "\n",
    "n_top_words = 20\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()  # Array mapping from index to name \n",
    "# tf_idf_feature_names = tf_idf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)\n",
    "# print_top_words(lda, tf_idf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    _ = topic.sum()\n",
    "    print('==========', 'theme #%d'%topic_idx, '==========')\n",
    "    target = topic.argsort()[::-1][:50]\n",
    "    formula = ''\n",
    "    for item in target:\n",
    "        print(tf_feature_names[item], ',', topic[item] / _)\n",
    "#         print('%s*%f'%(tf_feature_names[item], topic[item] / _))\n",
    "        formula += '\"%s\"*%f + '%(tf_feature_names[item], topic[item] / _)\n",
    "    print(formula.rstrip(' +'), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Interactive topic model      \n",
    "\n",
    "Interpretation of interactive visual topic model            \n",
    "Each circle on the left represents a theme. \n",
    "The size of the circle represents the proportion of articles covered by each topic.            \n",
    "Without hovering over the topic, the right-hand keyword represents the 30 most important keywords extracted from the entire text.            \n",
    "When hovering over a topic, the red progress bar on the right indicates the frequency of the corresponding keyword under the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "pyLDAvis.save_html(vis, 'vis.html')  \n",
    "# Because of the problem of visual class library, you need to save the reference path of CSS and JS after saving it as a web file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edited by:\n",
    "\n",
    "#### Yuru LI\n",
    "the Communication University of China\n",
    "\n",
    "Laboratory of Data Mining and Social Computing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
